{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65dfa4d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you're using Google Colab and not running locally, run this cell.\n",
    "\n",
    "## Install dependencies\n",
    "!pip install wget\n",
    "!apt-get install sox libsndfile1 ffmpeg portaudio19-dev\n",
    "!pip install unidecode\n",
    "!pip install pyaudio\n",
    "\n",
    "# ## Install NeMo\n",
    "BRANCH = 'r1.4.0'\n",
    "!python -m pip install git+https://github.com/NVIDIA/NeMo.git@$BRANCH#egg=nemo_toolkit[asr]\n",
    "\n",
    "## Install TorchAudio\n",
    "!pip install torchaudio==0.9.0\n",
    "\n",
    "## Grab the config we'll use in this example\n",
    "!mkdir configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a0168bc7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording ...\n",
      "Finished.\n"
     ]
    }
   ],
   "source": [
    "!python asr.py record myvoice.wav 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1e6eb7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python asr.py play myvoice.wav 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "84656089",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████████████████████████| 159/159 [00:00<00:00, 135kB/s]\n",
      "Downloading: 100%|██████████████████████████████| 291/291 [00:00<00:00, 387kB/s]\n",
      "Downloading: 100%|█████████████████████████████| 163/163 [00:00<00:00, 93.9kB/s]\n",
      "Downloading: 100%|████████████████████████████| 85.0/85.0 [00:00<00:00, 105kB/s]\n",
      "Downloading: 100%|█████████████████████████| 1.60k/1.60k [00:00<00:00, 2.06MB/s]\n",
      "Downloading: 100%|███████████████████████████| 378M/378M [00:11<00:00, 32.9MB/s]\n"
     ]
    }
   ],
   "source": [
    "!python asr.py transcribe myvoice.wav"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "51874f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python asr.py evaluate myvoice_transcription.txt ground-truth.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c538793a",
   "metadata": {},
   "source": [
    "1. How would you grade the models transcriptions of your voice (without considering WER)?\n",
    "\n",
    "I am not very satisfied with the translated text. There are a lot of gibberish in it, possibly because I don't pronounce it properly. Anyway, there are many others.\n",
    "\n",
    "2. Why is the character level error rate lower than at the word level?\n",
    "\n",
    "Because of the pauses in human speech, stopping would make it easier for the machine to distinguish the symbols.\n",
    "\n",
    "\n",
    "3. Are the models errors primarily on the word level, or is it missing phonetic sounds all together? Given this assessment, if you had to improve this model, would you focus primarily on the acoustic model or the language model?\n",
    "\n",
    "\n",
    "Some alliteration may have been missed. There are also some speaking accents that drag out the sound making it difficult for the machine to distinguish. For example, when listening to a word, it thinks it is \"litinin\". I think we should let the machine listen to the person to form a model, so that some accents and habits of the person will be recognized.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anly-580]",
   "language": "python",
   "name": "conda-env-anly-580-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
